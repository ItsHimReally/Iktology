{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d5a26cc-a81b-4c87-b3c7-893e3c461d38",
   "metadata": {},
   "source": [
    "###### Получаем базу знаний (просто папка нормативной документации из предоставленных данных)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97162f8b-ea3e-42d9-9036-96418ac808aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /home/user1/environments/venv/lib/python3.10/site-packages (5.2.0)\n",
      "Requirement already satisfied: tqdm in /home/user1/environments/venv/lib/python3.10/site-packages (from gdown) (4.66.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/user1/environments/venv/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: requests[socks] in /home/user1/environments/venv/lib/python3.10/site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: filelock in /home/user1/environments/venv/lib/python3.10/site-packages (from gdown) (3.16.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/user1/environments/venv/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user1/environments/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user1/environments/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user1/environments/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user1/environments/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/user1/environments/venv/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1vfU6ke9S-NCP9lo-164uYYZy7RNb1ZRl\n",
      "To: /home/user1/notebooks/knowlegebase.zip\n",
      "100%|██████████████████████████████████████| 2.63M/2.63M [00:00<00:00, 21.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown\n",
    "!gdown 1vfU6ke9S-NCP9lo-164uYYZy7RNb1ZRl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8e56c17-7275-4a41-9d06-abf8989a9b16",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  knowlegebase.zip\n",
      "   creating: knowlegebase/\n",
      "  inflating: knowlegebase/metodika_provedenija_inventarizacii_vybrosov_zagrj.md  \n",
      "  inflating: knowlegebase/utochn-metodika-metalloobrabotka-2021.md  \n",
      "  inflating: knowlegebase/ГОСТ Р 58577-2019. Национальный стандарт Российской Федераци.md  \n",
      "  inflating: knowlegebase/Закон от 04_05_1999 N 96-ФЗ Об охране атмосферного воздуха (с изменениями на 8 августа 2024 года)_Текст.md  \n",
      "  inflating: knowlegebase/Методика горных работ, Люберцы 1999.md  \n",
      "  inflating: knowlegebase/Методика определения выбросов загрязняющих веществ в атмосферу при сжигании топлива в котлах, Москва, 1999.md  \n",
      "  inflating: knowlegebase/Методика определения выбросов при сжигании топлива в котлах менее 20гкал в час.md  \n",
      "  inflating: knowlegebase/Методика по нормированию и определению выбросов загрязняющих веществ в атмосферу на предприятиях нефтепродуктообеспечения ОАО.md  \n",
      "  inflating: knowlegebase/Методика проведения инвентаризации выбросов для автотранспорт.md  \n",
      "  inflating: knowlegebase/МЕТОДИКА РАСЧЕТА ВЫБРОСОВ В АТМОСФЕРУ ДЛЯ ПРЕДРИЯТИЙ БЫТОВОГО ОБСЛУЖИВАНИЯ.md  \n",
      "  inflating: knowlegebase/Методика расчета выделений (выбросов) загрязняющих веществ в атмосферу при сварочных работах (на основе удельных показателей), НИИ.md  \n",
      "  inflating: knowlegebase/Методика расчета тепловозов.md  \n",
      "  inflating: knowlegebase/Методика-радиоэлектроника.md  \n",
      "  inflating: knowlegebase/Методические указания по определению выбросов загрязняющих веществ в атмосферу из резервуаров Новополоцк 1997.md  \n",
      "  inflating: knowlegebase/Методическое письмо НИИ Атмосфера 33533-07 от 17.05.2000.md  \n",
      "  inflating: knowlegebase/Письмо_ Минприроды России от 16.11.2021 N 20-47_35535.md  \n",
      "  inflating: knowlegebase/Постановление Главного государственного санитарного врача РФ (1).md  \n",
      "  inflating: knowlegebase/Постановление Главного государственного санитарного врача РФ (2).md  \n",
      "  inflating: knowlegebase/Постановление Главного государственного санитарного врача РФ.md  \n",
      "  inflating: knowlegebase/Постановление Правительства РФ от 31.12.2020 N 2398 (ред. от.md  \n",
      "  inflating: knowlegebase/Приказ Минприроды России от 06.06.2017 N 273  Об утверждении.md  \n",
      "  inflating: knowlegebase/Приказ Минприроды России от 11.08.2020 N 581  Об утверждении.md  \n",
      "  inflating: knowlegebase/Приказ Минприроды России от 19.11.2021 N 871  Об утверждении.md  \n",
      "  inflating: knowlegebase/Приказ Минприроды России от 28.11.2019 N 811  Об утверждении.md  \n",
      "  inflating: knowlegebase/Приказ от 06_06_2017 N 273 Об утверждении методов расчетов рассеивания выбросов вредных..._Текст.md  \n",
      "  inflating: knowlegebase/Приказ от 11_08_2020 N 581 Об утверждении методики разработки (расчета) и установления нормативов..._Текст.md  \n",
      "  inflating: knowlegebase/Приказ от 19_11_2021 N 871 Об утверждении Порядка проведения инвентаризации стационарных источников..._Текст.md  \n",
      "  inflating: knowlegebase/Распоряжение Правительства РФ от 08.07.2015 N 1316-р (ред. о.md  \n",
      "  inflating: knowlegebase/Учебник-Экология.md  \n",
      "  inflating: knowlegebase/Федеральный закон от 04.05.1999 N 96-ФЗ (ред. от 08.08.2024).md  \n",
      "  inflating: knowlegebase/Федеральный закон от 10.01.2002 N 7-ФЗ (ред. от 08.08.2024).md  \n",
      "  inflating: knowlegebase/Экология Учебник.md  \n"
     ]
    }
   ],
   "source": [
    "#!sudo apt install zip\n",
    "!unzip knowlegebase.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69485553-012c-4b19-8c71-fff229037ef1",
   "metadata": {},
   "source": [
    "## Начнем с QA модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c2607210-e16d-41ff-92fc-f0c348ba1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f172369-c9db-42d8-96a4-9d81ce9911c7",
   "metadata": {},
   "source": [
    "Подразумевается что текущий ноутбук и папка test_dataset_iktin_test на одном уровне"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1040bedb-6515-4b33-9b44-28cba4728bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_dataset_iktin_test/Иктин test/Данные для тестирования/test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c46ee63f-e203-4efc-9815-d3644707b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 32/32 [00:00<00:00, 388.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Читаем директорию с нормативной документацией\n",
    "loader = DirectoryLoader(\n",
    "    './knowlegebase/',\n",
    "    glob=\"./*.md\",\n",
    "    loader_cls=TextLoader,\n",
    "    show_progress=True,\n",
    "    use_multithreading=True,\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5770661-23e3-4374-bbfa-0531db1d376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим тексты для поиска через bm25\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d87d8c43-2d5f-4c82-b8d5-7b2952cc661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружаем модель для эмбедингов текста и создаем векторную базу данных для поиска\n",
    "embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name='TatonkaHF/bge-m3_en_ru',\n",
    "    model_kwargs={\"device\": \"cuda\"}\n",
    ")\n",
    "vectordb = FAISS.from_documents(\n",
    "    documents = texts,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba485e92-660e-4b1b-a8f7-54836e20b432",
   "metadata": {},
   "source": [
    "#### Используем модель phi3, длина контекста и размера кэша понижены, чтобы помещаться в видеопамять (tesla v100 32gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4481357b-1d1c-4129-9e97-6d4bb503dfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "llm = LLM(model=MODEL_NAME, dtype='half', max_num_seqs=1, gpu_memory_utilization=0.8, max_model_len=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542de2e0-f90d-4c32-8baa-55f812f1881f",
   "metadata": {},
   "source": [
    "##### Создание ретриверов bm25 и bge работают вместе для повышения качетсво поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "71af0f9d-c1dd-4582-9673-9f4497e14a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "retriever_base = BM25Retriever.from_documents(texts, k=k//2)\n",
    "retriever_advanced = vectordb.as_retriever(search_kwargs={\"k\": k//2, \"search_type\": \"similarity\"})\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[retriever_base, retriever_advanced], weights=[0.5, 0.5], k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dfd39f7a-0608-4f84-8f42-735de3af6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Небольшая настройка параметров для понижения галюцинаций системы\n",
    "sampling_params = SamplingParams(temperature=0.01, repetition_penalty=0.8, frequency_penalty=1.2, max_tokens=2048, min_tokens=10, top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8b9c0a-71ca-4245-bd7a-dbebce4b9c95",
   "metadata": {},
   "source": [
    "##### Для начала обработаем общие вопросы не требующие файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "191aa65c-ca87-4855-8781-c786cdc13863",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = test_df[test_df['Документ']=='Нет']['Вопрос']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f0bac74-e362-4001-92d7-7d7820877e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ad933-e8ce-499c-9d5d-cd2f2f5e98c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, question in tqdm(enumerate(questions), total=len(questions)):\n",
    "    retriever_ans_raw = ensemble_retriever.invoke(question, k=k)\n",
    "    page_content = [i.page_content for i in retriever_ans_raw]\n",
    "    retriever_ans = '\\n\\n'.join(page_content)\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        Твоя задача отвечать на вопросы пользователей связанные с экологией,\n",
    "        тебе будет предоставлен контекст ответчай строго по нему.\n",
    "\n",
    "        Контекст из которого необходимо взять информацию:\n",
    "        {retriever_ans}\n",
    "\n",
    "        ИНФОРМАЦИЮ ДЛЯ ОТВЕТА БЕРИ ТОЛЬКО ИЗ КОНТЕКСТА ВЫШЕ, ЕСЛИ ЕЕ ТАМ НЕТ НАПИШИ ИНФОРМАЦИЯ ОТСУТВУЕТ\n",
    "        \"\"\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question,\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    output = llm.generate(prompt, sampling_params)\n",
    "    ans[i+1] = output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0071449-0265-4aef-b7c8-3daa3a5d3861",
   "metadata": {},
   "source": [
    "##### Очистка файлов для ответов на конкретные вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "db634435-3a5d-4a14-86cf-4cdd37eb1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_with_files = test_df[test_df['Документ']!='Нет'][['Вопрос', 'Документ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9c97e-96cf-494c-b018-4eda7df97718",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# бибилотеку необходимо установить \n",
    "# sudo apt libreoffice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfb03cb-1f58-4d76-ae59-22ae1d334311",
   "metadata": {},
   "source": [
    "##### Класс реализовывает в себе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "44ac8617-6f9e-432e-926b-385f38286310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer, LTChar, LTFigure\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "from pdf2image import convert_from_path\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import PurePosixPath\n",
    "\n",
    "class PDFScanner: # Класс отвечает за сканиорвание ПДФ-Документов\n",
    "    CATEGORY_UPPER_BOUND = 3000\n",
    "    NAME_UPPER_BOUND = 3000\n",
    "    PATTERNS = {\n",
    "        r\"[\\d,]*\\n|\\n.{1,2}\\n\": r\"\\n\",\n",
    "        r\"\\n[\\d,.=]{1,10}\": r\"\\n\",\n",
    "        r\"^\\s*\\S*[^|]\\s*$\": r\"\",\n",
    "        r\"\f",
    "\": \"\",\n",
    "        r\" *\\n+\": r\"\\n\"\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def scan_full(path) -> tuple[str, str, str]: # Скан документа с выдержкой краткого названия и категории\n",
    "        pdf = PDFScanner.convert(path)\n",
    "        text = PDFScanner.extract_text(pdf)\n",
    "        category = categorize(text[0:PDFScanner.CATEGORY_UPPER_BOUND])\n",
    "        delay = datetime.now() + timedelta(seconds=2)\n",
    "        while datetime.now() < delay:\n",
    "            pass\n",
    "        name = rename(text[0:PDFScanner.NAME_UPPER_BOUND])\n",
    "        return (category, name, text)\n",
    "\n",
    "    @staticmethod\n",
    "    def scan_partial(path) -> str: # Частичное сканирование документа\n",
    "        pdf = PDFScanner.convert(path)\n",
    "        text = PDFScanner.extract_text(pdf)\n",
    "        return text\n",
    "        \n",
    "    @staticmethod\n",
    "    def clean_text(text) -> str: # Очистить текст от артефактов\n",
    "        for i in PDFScanner.PATTERNS:\n",
    "            text = re.sub(i, PDFScanner.PATTERNS[i], text, flags=re.MULTILINE)\n",
    "        return text\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_text(path) -> str: # Получение текста и таблиц\n",
    "        pdf_read = PdfReader(path)\n",
    "        pages_dict = {} \n",
    "        for pagenum, page in enumerate(extract_pages(path)): # Проходимся по каждой странице\n",
    "            page_obj = pdf_read.pages[pagenum]\n",
    "            text_from_tables = []\n",
    "            page_content = []\n",
    "            table_in_page= -1\n",
    "            pdf = pdfplumber.open(path)\n",
    "            page_tables = pdf.pages[pagenum]\n",
    "\n",
    "            tables = page_tables.find_tables() # Найти кол-во таблиц на странице\n",
    "            if len(tables)!=0:\n",
    "                table_in_page = 0\n",
    "\n",
    "            # Извлечение таблиц\n",
    "            for table_num in range(len(tables)):\n",
    "                table = PDFScanner.extract_table(path, pagenum, table_num)\n",
    "                table_string = PDFScanner.table_converter(table)\n",
    "                text_from_tables.append(table_string)\n",
    "\n",
    "            # Поиск элементов страницы\n",
    "            page_elements = [(element.y1, element) for element in page._objs]\n",
    "            # Сортировка в порядке их появления\n",
    "            page_elements.sort(key=lambda a: a[0], reverse=True)\n",
    "\n",
    "\n",
    "            scan_images = True # Сканировать картинки OCR'ом или нет\n",
    "            for i, component in enumerate(page_elements): # Проходимся по каждому компоненту страницы\n",
    "                element = component[1]\n",
    "\n",
    "                # Check the elements for tables\n",
    "                if table_in_page == -1:\n",
    "                    pass\n",
    "                else:\n",
    "                    if PDFScanner.is_element_inside_any_table(element, page ,tables):\n",
    "                        table_found = PDFScanner.find_table_for_element(element,page ,tables)\n",
    "                        if table_found == table_in_page and table_found != None:    \n",
    "                            table_in_page+=1\n",
    "                        continue\n",
    "\n",
    "                if not PDFScanner.is_element_inside_any_table(element,page,tables):\n",
    "                    \n",
    "                    # Если элемент - текстовое поле\n",
    "                    if isinstance(element, LTTextContainer):\n",
    "                        line_text = PDFScanner.extract_line(element)\n",
    "                        # Append the format for each line containing text\n",
    "                        page_content.append(line_text)\n",
    "                        scan_images = False # Если увидели текст, то больше не сканируем картинки\n",
    "                    continue\n",
    "                    # Check the elements for images\n",
    "                    if  scan_images and isinstance(element, LTFigure):\n",
    "                        print(\"Scanning Image...\")\n",
    "                        # Crop the image from PDF\n",
    "                        PDFScanner.crop_image(element, page_obj)\n",
    "                        # Convert the croped pdf to image\n",
    "                        PDFScanner.convert_to_images('cropped_image.pdf')\n",
    "                        # Extract the text from image\n",
    "                        image_text = PDFScanner.image_to_text('PDF_image.png')\n",
    "                        page_content.append(image_text)\n",
    "                        # Add a placeholder in the text and format lists\n",
    "\n",
    "            # Create the key of the dictionary          \n",
    "            dctkey = pagenum\n",
    "            # Add the list of list as value of the page key\n",
    "            pages_dict[dctkey]= [text_from_tables, page_content]\n",
    "\n",
    "        md = \"\"\n",
    "        for i in range(len(pages_dict)):\n",
    "            for j in range(len(pages_dict[i][1])): # Добавление блоков текста\n",
    "                md += PDFScanner.clean_text(pages_dict[i][1][j]) + '\\n'\n",
    "            for j in range(len(pages_dict[i][0])): # Добавление блоков таблиц\n",
    "                md += pages_dict[i][0][j] + '\\n'\n",
    "        md = re.sub(r\" *\\n+\", PDFScanner.PATTERNS[r\" *\\n+\"], md, flags=re.MULTILINE)\n",
    "        return md\n",
    "\n",
    "    # Create a function to crop the image elements from PDFs\n",
    "    @staticmethod\n",
    "    def crop_image(element, pageObj):\n",
    "        # Get the coordinates to crop the image from PDF\n",
    "        [image_left, image_top, image_right, image_bottom] = [element.x0,element.y0,element.x1,element.y1] \n",
    "        # Crop the page using coordinates (left, bottom, right, top)\n",
    "        pageObj.mediabox.lower_left = (image_left, image_bottom)\n",
    "        pageObj.mediabox.upper_right = (image_right, image_top)\n",
    "        # Save the cropped page to a new PDF\n",
    "        cropped_pdf_writer = PdfWriter()\n",
    "        cropped_pdf_writer.add_page(pageObj)\n",
    "        # Save the cropped PDF to a new file\n",
    "        with open('cropped_image.pdf', 'wb') as cropped_pdf_file:\n",
    "            cropped_pdf_writer.write(cropped_pdf_file)\n",
    "\n",
    "    # Create a function to convert the PDF to images\n",
    "    @staticmethod\n",
    "    def convert_to_images(input_file,):\n",
    "        images = convert_from_path(input_file)\n",
    "        image = images[0]\n",
    "        output_file = 'PDF_image.png'\n",
    "        image.save(output_file, 'PNG')\n",
    "\n",
    "    # Create a function to read text from images\n",
    "    @staticmethod\n",
    "    def image_to_text(image_path):\n",
    "        # Read the image\n",
    "        img = Image.open(image_path)\n",
    "        # Extract the text from the image\n",
    "        text = pytesseract.image_to_string(img, lang='rus')\n",
    "        return text\n",
    "\n",
    "    # Create function to extract text\n",
    "    @staticmethod\n",
    "    def extract_line(element):\n",
    "        # Extracting the text from the in line text element\n",
    "        line_text = element.get_text()\n",
    "        return line_text\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_table(pdf_path, page_num, table_num):\n",
    "        # Open the pdf file\n",
    "        pdf = pdfplumber.open(pdf_path)\n",
    "        # Find the examined page\n",
    "        table_page = pdf.pages[page_num]\n",
    "        # Extract the appropriate table\n",
    "        table = table_page.extract_tables()[table_num]\n",
    "        return table\n",
    "\n",
    "    # Convert table into appropriate fromat\n",
    "    @staticmethod\n",
    "    def table_converter(table):\n",
    "        # table_string = '\\n'\n",
    "        table_string = ''\n",
    "        # Iterate through each row of the table\n",
    "        for row_num in range(len(table)):\n",
    "            row = table[row_num]\n",
    "            # Remove the line breaker from the wrapted texts\n",
    "            cleaned_row = [item.replace('\\n', ' ') if item is not None and '\\n' in item else '' if item is None else item for item in row]\n",
    "            if row_num == 2:\n",
    "                cleaned_row = ['-' for _ in row] \n",
    "                table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
    "                continue\n",
    "            if any(cleaned_row):\n",
    "                # Convert the table into a string \n",
    "                table_string+=('|'+'|'.join(cleaned_row)+'|'+'\\n')\n",
    "        # Removing the last line break\n",
    "        table_string = table_string[:-1]\n",
    "        return table_string\n",
    "\n",
    "    # Create a function to check if the element is in any tables present in the page\n",
    "    @staticmethod\n",
    "    def is_element_inside_any_table(element, page ,tables):\n",
    "        x0, y0up, x1, y1up = element.bbox\n",
    "        # Change the cordinates because the pdfminer counts from the botton to top of the page\n",
    "        y0 = page.bbox[3] - y1up\n",
    "        y1 = page.bbox[3] - y0up\n",
    "        for table in tables:\n",
    "            tx0, ty0, tx1, ty1 = table.bbox\n",
    "            if tx0 <= x0 <= x1 <= tx1 and ty0 <= y0 <= y1 <= ty1:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # Function to find the table for a given element\n",
    "    @staticmethod\n",
    "    def find_table_for_element(element, page ,tables):\n",
    "        x0, y0up, x1, y1up = element.bbox\n",
    "        # Change the cordinates because the pdfminer counts from the botton to top of the page\n",
    "        y0 = page.bbox[3] - y1up\n",
    "        y1 = page.bbox[3] - y0up\n",
    "        for i, table in enumerate(tables):\n",
    "            tx0, ty0, tx1, ty1 = table.bbox\n",
    "            if tx0 <= x0 <= x1 <= tx1 and ty0 <= y0 <= y1 <= ty1:\n",
    "                return i  # Return the index of the table\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert(path):\n",
    "        if PurePosixPath(path).stem == \"pdf\":\n",
    "            return path\n",
    "        os.system(f\"libreoffice --headless --convert-to pdf \\\"{path}\\\"\")\n",
    "        return PurePosixPath(path).stem + \".pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "755f4c77-f43b-4e9e-8410-5db4830ddd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathqa = 'test_dataset_iktin_test/Иктин test/Данные для тестирования/Проект 2 (для QnA)/Проект Word/'\n",
    "book1 = pathqa + 'Том 1 Инвентаризация Эко Агро.docx'\n",
    "book2 = pathqa + 'Том 2 ПДВ Эко Агро.docx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93548ccb-a57b-4085-b58b-6be31bf4550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_dataset_iktin_test/process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c34937a-339d-42d5-a1fe-f74961a4309c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "convert /home/user1/notebooks/test_dataset_iktin_test/Иктин test/Данные для тестирования/Проект 2 (для QnA)/Проект Word/Том 1 Инвентаризация Эко Агро.docx -> /home/user1/notebooks/Том 1 Инвентаризация Эко Агро.pdf using filter : writer_pdf_Export\n",
      "convert /home/user1/notebooks/test_dataset_iktin_test/Иктин test/Данные для тестирования/Проект 2 (для QnA)/Проект Word/Том 2 ПДВ Эко Агро.docx -> /home/user1/notebooks/Том 2 ПДВ Эко Агро.pdf using filter : writer_pdf_Export\n"
     ]
    }
   ],
   "source": [
    "# обработка длится долго из-за чтения таблиц в markdown (до 5 минут из-за локльного перевода)\n",
    "book1_text = PDFScanner.scan_partial(book1)\n",
    "with open(\"test_dataset_iktin_test/process/book1_qa.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(book1_text)\n",
    "\n",
    "book2_text = PDFScanner.scan_partial(book2)\n",
    "with open(\"test_dataset_iktin_test/process/book2_qa.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(book2_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33252ba3-0004-467e-9535-75b1a4868f92",
   "metadata": {},
   "source": [
    "###### На сайте и в fastapi все реализованно через сессии, здесь в силу не хватки времени обработаем отдельно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48a61c2c-d430-4a37-a6e3-61cf08bbf24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book1_doc = Document(\n",
    "    page_content=book1_text,\n",
    "    metadata={\"source\": 'book1'}\n",
    ")\n",
    "\n",
    "texts_book1 = text_splitter.split_documents([book1_doc])\n",
    "\n",
    "vectordb_book1 = FAISS.from_documents(\n",
    "    documents = texts_book1,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "69e36a95-7f33-4e65-9ff5-3c790f94d885",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "retriever_base_book1 = BM25Retriever.from_documents(texts_book1, k=k//2)\n",
    "retriever_advanced_book1 = vectordb_book1.as_retriever(search_kwargs={\"k\": k//2, \"search_type\": \"similarity\"})\n",
    "ensemble_retriever_book1 = EnsembleRetriever(retrievers=[retriever_base_book1, retriever_advanced_book1], weights=[0.5, 0.5], k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1562b700-24ab-41f4-8d82-8f688f38258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "book2_doc = Document(\n",
    "    page_content=book1_text,\n",
    "    metadata={\"source\": 'book2'}\n",
    ")\n",
    "\n",
    "texts_book2 = text_splitter.split_documents([book2_doc])\n",
    "\n",
    "vectordb_book2 = FAISS.from_documents(\n",
    "    documents = texts_book2,\n",
    "    embedding = embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0220298d-dfaf-4c26-9f35-804f42a979f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "retriever_base_book2 = BM25Retriever.from_documents(texts_book2, k=k//2)\n",
    "retriever_advanced_book2 = vectordb_book2.as_retriever(search_kwargs={\"k\": k//2, \"search_type\": \"similarity\"})\n",
    "ensemble_retriever_book2 = EnsembleRetriever(retrievers=[retriever_base_book2, retriever_advanced_book2], weights=[0.5, 0.5], k=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019150c6-807d-4bc5-8b57-1022862e38fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i, info in tqdm(questions_with_files.iterrows(), total=len(questions_with_files)):\n",
    "    doc = info['Документ']\n",
    "    question = info['Вопрос']\n",
    "\n",
    "    if doc.startswith('Книга 1'):\n",
    "        retriever_ans_raw = ensemble_retriever_book1.invoke(question, k=k)\n",
    "    else:\n",
    "        retriever_ans_raw = ensemble_retriever_book2.invoke(question, k=k)\n",
    "\n",
    "    page_content = [i.page_content for i in retriever_ans_raw]\n",
    "    retriever_ans = '\\n\\n'.join(page_content)\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        Твоя задача отвечать на вопросы пользователей связанные с экологией,\n",
    "        тебе будет предоставлен контекст ответчай строго по нему.\n",
    "\n",
    "        Контекст из которого необходимо взять информацию:\n",
    "        {retriever_ans}\n",
    "\n",
    "        ИНФОРМАЦИЮ ДЛЯ ОТВЕТА БЕРИ ТОЛЬКО ИЗ КОНТЕКСТА ВЫШЕ, ЕСЛИ ЕЕ ТАМ НЕТ НАПИШИ ИНФОРМАЦИЯ ОТСУТВУЕТ\n",
    "        \"\"\"\n",
    "    }, {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": question,\n",
    "    }], tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    output = llm.generate(prompt, sampling_params)\n",
    "    ans[i+1] = output[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "be3f3e81-6e0c-4762-b88d-f56633a6c9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(ans.items()), columns=['№', 'answer'])\n",
    "\n",
    "df['answer'] = df['answer'].str.replace('\\n', '', regex=False) # убираем \\n потому что модель отвечает в markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe14e929-0052-478f-92d2-120167818a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submit.csv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6febe6f1-d4d7-4fc1-8561-d5273060e03a",
   "metadata": {},
   "source": [
    "## Суммаризация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8014d2-8ea0-429c-9c9d-7537b207c9e2",
   "metadata": {},
   "source": [
    "На сайте реализована суммаризация всех подгружаемых файлов, подробнее можно ознакомится там"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0476a277-3c5a-49d9-9f91-f87dfb7bdcf1",
   "metadata": {},
   "source": [
    "Из-за особенностей библиотеки pytesseract, которая требует явно указать путь к библиотеке, аналогично для windows pytesseract.pytesseract.tesseract_cmd = 'C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe'. Еще поддержка есть на нашем fastapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "153858cb-cffe-45ec-92c9-0105acf592c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1lsw-3qYL6DTEG2ut1WI3nVY-dTCDmnu_\n",
      "To: /home/user1/notebooks/Книга_1_Инвентаризация_Био_Агро_Дон_1.pdf\n",
      "100%|██████████████████████████████████████| 22.8M/22.8M [00:00<00:00, 73.7MB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1r23qz36CNy6PJVWbDRgCVJp6Hmixo3wj\n",
      "To: /home/user1/notebooks/Книга 1 ПДВ Био Агро Дон.pdf\n",
      "100%|██████████████████████████████████████| 12.5M/12.5M [00:00<00:00, 50.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 1lsw-3qYL6DTEG2ut1WI3nVY-dTCDmnu_\n",
    "!gdown 1r23qz36CNy6PJVWbDRgCVJp6Hmixo3wj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65671086-181e-4d28-9b82-c13318ad65d9",
   "metadata": {},
   "source": [
    "Суммаризация в отличае от других частей нашего проекта работает через YandexGPT. Ниже приведен код который поможет получить эту суммаризцию. Вы можете добавить свои ключи или перейти на наш сайт http://iktin.tw1.su/docs/ по этому адресу в категории Тест находятся файлы предложеные нам для суммаризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87f2616-5198-4831-93b1-5ee8a8a1654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "SYSTEM_SUM = \"\"\"\n",
    "Шаблон структуры характеристик предприятия\n",
    "1. Основной вид деятельности\n",
    "2. Основное структурное подразделение\n",
    "3. Процесс разработки\n",
    "4. Склады хранения\n",
    "5. Заправка техники\n",
    "6. Выбросы загрязняющих веществ (ЗВ)\n",
    "\n",
    "Проанализируй текст ниже и просуммаризируй его, составив параграф, структура которого приведена выше. Старайся меньше придумывать и больше придерживаться текста документа.\n",
    "\"\"\"\n",
    "SUMMARIZER = YA_GPT(SYSTEM_SUM)\n",
    "\n",
    "\n",
    "def summarize(text):\n",
    "    return SUMMARIZER.query(text)\n",
    "\n",
    "load_dotenv()\n",
    "IAM_TOKEN = os.getenv(\"IAM_TOKEN\")\n",
    "FOLDER_ID = os.getenv(\"FOLDER_ID\")\n",
    "\n",
    "\n",
    "class YA_GPT():\n",
    "    URL = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
    "    IAM_TOKEN = os.getenv(\"IAM_TOKEN\")\n",
    "    FOLDER_ID = os.getenv(\"FOLDER_ID\")\n",
    "\n",
    "    def __init__(self, system) -> None:\n",
    "        self.system = system\n",
    "        self.headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Api-Key {YA_GPT.IAM_TOKEN}\",\n",
    "        }\n",
    "\n",
    "    def query(self, prompt):\n",
    "        query = {\n",
    "            \"modelUri\": f\"gpt://{YA_GPT.FOLDER_ID}/yandexgpt-32k/rc\",\n",
    "            \"completionOptions\": {\n",
    "                \"stream\": False,\n",
    "                \"temperature\": 0.6,\n",
    "                \"maxTokens\": \"2000\"\n",
    "            },\n",
    "            \"messages\": [\n",
    "                {\n",
    "                \"role\": \"system\",\n",
    "                \"text\": self.system\n",
    "                },\n",
    "                {\n",
    "                \"role\": \"user\",\n",
    "                \"text\": prompt\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        response = requests.post(YA_GPT.URL, headers=self.headers, json=query).json()\n",
    "        print(response)\n",
    "        return response[\"result\"][\"alternatives\"][0][\"message\"][\"text\"]\n",
    "\n",
    "summarize(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
